{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nanoGPT trained on Harry Potter Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem  Statement"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Creating a character-level GPT on the books of Harry Potter.\n",
    "2. Fine-tuning GPT-2(created by Karpathy) on the books of Harry Potter.\n",
    "\n",
    "This project is a slight modification of a fork on Karpathy's work - https://github.com/karpathy/nanoGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was downloaded from - https://github.com/priya-dwivedi/Deep-Learning/tree/master/GPT2-HarryPotter-Training/books\n",
    "\n",
    "All files were then merged and fed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the file\n",
    "with open('harry_potter_series.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  6765168\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "THE BOY WHO LIVED \n",
      "\n",
      "Mr. and Mrs. Dursley, of number four, Privet Drive, \n",
      "were proud to say that they were perfectly normal, \n",
      "thank you very much. They were the last people you’d \n",
      "expect to be involved in anything strange or \n",
      "mysterious, because they just didn’t hold with such \n",
      "nonsense. \n",
      "\n",
      "Mr. Dursley was the director of a firm called \n",
      "Grunnings, which made drills. He was a big, beefy \n",
      "man with hardly any neck, although he did have a \n",
      "very large mustache. Mrs. Dursley was thin and \n",
      "blonde and had nearly twice the usual amount of \n",
      "neck, which came in very useful as she spent so \n",
      "much of her time craning over garden fences, spying \n",
      "on the neighbors. The Dursley s had a small son \n",
      "called Dudley and in their opinion there was no finer \n",
      "boy anywhere. \n",
      "\n",
      "The Dursleys had everything they wanted, but they \n",
      "also had a secret, and their greatest fear was that \n",
      "somebody would discover it. They didn’t think they \n",
      "could bear it if anyone found out about the Potters. \n",
      "Mrs. Potter was Mrs. Dursl\n"
     ]
    }
   ],
   "source": [
    "# let's look at the first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !\"%&'()*,-./0123456789:;>?ABCDEFGHIJKLMNOPQRSTUVWXYZ\\]abcdefghijklmnopqrstuvwxyz|~—‘’“”•■□\n",
      "92\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodology"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a character-level GPT model from scratch\n",
    "In order to utilize this project we had to follow these steps:\n",
    " - import the new text containing J.K. Rowling's books,\n",
    " - tweak and run the **prepare.py** script provided by Karpathy in the **'data/shakespeare_char'** folder, <br>\n",
    "   > This script transforms each charachter to an integer and creats the a map to later decode it, it also generates train and validation sets saving them in '.bin' files.\n",
    " - run the tweaked script in order to generate train and validation sets,\n",
    " - tweak a copy of **'train_shakespeare_char.py'** (called **'train_harrypotter_char.py'**),<br>\n",
    "   > This is a configuration file, loaded by the **'configurator.py'** script when the **train.py** script is properly launched like so **'python train.py .\\config\\train_harrypotter_char.py'**. In particular we reduced the number of iterations from 5000 to 2500, increased the learning rate to 1e-2 (0.01), increase the minimum learning rate to 1e-3, and reduced the batch size to 32 from the original 64. This last change was necessary due to hardware constraints as our GPU quickly run out of memory with a batch size of 64.\n",
    " - run the **'train.py'** script utilizing our configuration file as mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 10.66M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from model import GPTConfig, GPT\n",
    "\n",
    "ckpt_path = os.path.join('../out-harrypotter-char', 'ckpt.pt')\n",
    "checkpoint = torch.load(ckpt_path, map_location='cuda')\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "model = GPT(gptconf)\n",
    "state_dict = checkpoint['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see this 'small' network is already quite complex with almost 10.7M parameters. It should not surprise that our GPU run at full capacity for over 4 hours to process this training.\n",
    "\n",
    "**A. GPU usage graphs**\n",
    "<img src=\"GPU_usage.png\" alt=\"GPU usage\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every 250 iterations the model is evaluated and if the validation loss is reduced the new model is stored in **'out-harrypotter-char'**. To extract results from our models we can run **'python sample.py --out_dir=out-harrypotter-char'**, this will utilize the model saved to generate text starting from a '\\n' character."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetune a pretrained GPT2 model on Harry Potter books\n",
    "Fine tuning a GPT2 model is a very different endeavour, we initially planned to finetune all different model of GPT2, from the smallest, to the XL model. As we can see the size of the smallest model is already outstanding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 123.65M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_path = os.path.join('../out-harrypotter', 'ckpt.pt')\n",
    "checkpoint = torch.load(ckpt_path, map_location='cuda')\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "model = GPT(gptconf)\n",
    "state_dict = checkpoint['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already at the smallest size there is a order of magnitude difference with the previous model with over 120 million parameters.<br>\n",
    "Furthermore these models use by default a block size of 1024, which represent the attention. Unfortunately our available computational power and memory storage does not allow for these models to train or finetune. We tweaked the settings in order to reduce block size to 512, the model does automatically adjust to this size reduction through the 'crop_block_size' function defined in the model.py class within the GPT class.<br>\n",
    "Fine tuning a pretrained model adds one more step to the previous outlined process which is loading the weights and biases, the process is then as follows:<br>\n",
    " - import the new text containing J.K. Rowling's books to the **'data/harrypotter'** folder,\n",
    " - tweak and run the **prepare.py** script provided by Karpathy in the **'data/shakespeare'** folder, <br>\n",
    "   > This script transforms each sub-word token to an integer and creats the a map to later decode it, it also generates train and validation sets saving them in '.bin' files.\n",
    " - run the tweaked script in order to generate train and validation sets,\n",
    " - tweak a copy of **'finetune_shakespeare.py'** (called **'finetune_harrypotter.py'**),<br>\n",
    "   > This is a configuration file, loaded by the **'configurator.py'** script when the train.py script is properly launched like so **'python train.py .\\config\\finetune_harrypotter.py'**. In particular we reduced determined the reduction in block size from 1024 to 512, and change the model to gpt2 instead of gpt2-xl. Both changes were necessary due to hardware constraints. We also increased the number of iterations to 100.\n",
    " - run the **'train.py'** script utilizing our configuration file as mentioned above."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A. GPU usage graphs**\n",
    "<img src=\"GPU_usage_finetune.png\" alt=\"GPU usage\" />\n",
    "\n",
    "As we can see the GPU memory was fully utilized from start to finish even with this smaller GPT2 model but slightly less time was spent accessing that memory compared to the character level model previously built."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following graphs were created on https://wandb.ai/ platform. \n",
    "\n",
    "We used this platform to keep track of our system metrics, predictions and losses\n",
    "\n",
    "#### Graphs for Character-level GPT:\n",
    "\n",
    "**A. Iteration vs Step**\n",
    "<img src=\"harrypotter_char_iter.png\" alt=\"Iteration vs Step\" />\n",
    "\n",
    "**B. Train loss vs Step**\n",
    "<img src=\"harrypotter_char_train_loss.png\" alt=\"Train loss vs Step\" />\n",
    "\n",
    "**C. Val loss vs Step**\n",
    "<img src=\"harrypotter_char_val_loss.png\" alt=\"Val loss vs Step\" />\n",
    "\n",
    "#### Graphs for pre-trained GPT-2:\n",
    "\n",
    "**A. Iteration vs Step**\n",
    "<img src=\"harrypotter_gpt2_iter.png\" alt=\"Iteration vs Step\" />\n",
    "\n",
    "**B. Train loss vs Step**\n",
    "<img src=\"harrypotter_gpt2_train_loss.png\" alt=\"Train loss vs Step\" />\n",
    "\n",
    "**C. Val loss vs Step**\n",
    "<img src=\"harrypotter_gpt2_val_loss.png\" alt=\"Val loss vs Step\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Finetuning of GPT-2 gave results that were more understandable from the start and became more relevant to Harry Potter as iterations increases. \n",
    "Following is the output comparison of **GPT-2 after 5 iteration vs 100 iteration:**\n",
    "\n",
    "<img src=\"gpt2_5iter.png\" alt=\"gpt2_5iter\" width=\"500\" /> <img src=\"gpt2_100iter.png\" alt=\"gpt2_100iter\" width=\"500\"/>\n",
    "\n",
    "2. The model which was built from scratch (Character-level GPT) was barely readable at the begining but it's Harry Potter's influence was immediately clear.\n",
    "Following is the output comparison of **Character-level GPT after 250 iteration vs 2000 iteration:**\n",
    "\n",
    "<img src=\"char_gpt_250iter.png\" alt=\"char_gpt_250iter\" width=\"500\" /> <img src=\"char_gpt_2000iter.png\" alt=\"char_gpt_2000iter\" width=\"500\"/>\n",
    "\n",
    "3. Finetuning (GPT-2) improved it's relevance to Harry Potter books over the iterations while the model built from the scratch (Character-level GPT) became more readable and started to vageuly make sense.\n",
    "Following is the output comparison of **best of two models i.e. GPT-2 after 100 iteration vs Character-level GPT after 2000 iteration:**\n",
    "\n",
    "<img src=\"gpt2_100iter.png\" alt=\"gpt2_100iter\" width=\"500\"/> <img src=\"char_gpt_2000iter.png\" alt=\"char_gpt_2000iter\" width=\"500\"/>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refrences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/karpathy/nanoGPT\n",
    "\n",
    "https://github.com/priya-dwivedi/Deep-Learning/tree/master/GPT2-HarryPotter-Training/books"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
